{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7539dc5-46e6-4740-890e-0b233f8a3534",
   "metadata": {},
   "source": [
    "## Load the extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103e7975-8774-457c-8ef1-162573ca6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport -sparkmagic # it loses the references to the sessions if it reloads\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddde0b3a-a5b3-4e9c-8a97-0fe590e79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext livy_uploads.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8270623-2d54-4bdb-9c8d-80e76bdecf5f",
   "metadata": {},
   "source": [
    "## Fetching remote variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46dfcbed-0a59-4533-bf3c-a1922ae663ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HiveContext', 'StreamingContext', '__builtins__', 'cloudpickle', 'sc', 'spark', 'sqlContext']"
     ]
    }
   ],
   "source": [
    "print(list(sorted(globals())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1a1a1a-6777-4e4b-a5c0-dee6c15657cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_i4', '_ih', '_ii', '_iii', '_oh', 'display_dataframe', 'exit', 'get_ipython', 'ip', 'quit']\n"
     ]
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "print(list(sorted(globals())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46582d08-4d7d-45a5-9dab-a59ae4a037f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2025, 1, 25, 20, 38, 51, 265358, tzinfo=datetime.timezone(datetime.timedelta(0), 'UTC'))"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().astimezone()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bdda1f-5e64-4dd4-ace7-aa7ca1349910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%get_obj_from_spark -n now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc84d9e-d1c2-4cc2-8f5b-a999e53d9a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 1, 25, 20, 38, 51, 265358, tzinfo=datetime.timezone(datetime.timedelta(0), 'UTC'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492ba5f0-1df9-472e-8bd9-e06d7af2b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "delta = (datetime.now().astimezone() - now)\n",
    "assert delta.total_seconds() < 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e7b51-1a37-45ea-acc0-7be7ac355d3f",
   "metadata": {},
   "source": [
    "## Sending local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343468f8-6c26-4439-abf4-b02525785cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "foo = {2, 3, 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43925e6-ed31-4421-ad1a-d431d7d8fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%send_obj_to_spark -n foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea7a1719-7cef-4e3c-be92-f0c25211054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo_total = sum(foo)\n",
    "\n",
    "assert foo == {2, 3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49925f03-75bc-4fdc-8040-5e6f7bf0a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%get_obj_from_spark -n foo_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab5942b-943d-4610-90f9-ed88f0bb07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "assert foo_total == 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaa787-5d90-4a70-86bd-fd0224f27718",
   "metadata": {},
   "source": [
    "## Running commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89dabe55-534a-4f6d-a6a4-641df6c322df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72K\n",
      "drwxrwxrwt  1 root root 4.0K Jan 25 20:38 ./\n",
      "drwxr-xr-x  1 root root 4.0K Jan 25 20:36 ../\n",
      "-rw-------  1 app  app   22K Jan 25 20:38 1852999028476163381\n",
      "drwxr-xr-x  2 app  app  4.0K Jan 25 20:38 blockmgr-c96a4538-43d5-4b7e-9bb0-da3bb68ffbc6/\n",
      "drwxr-xr-x  2 app  app  4.0K Jan 25 20:38 hsperfdata_app/\n",
      "drwxr-xr-x  2 root root 4.0K Jul 25  2024 hsperfdata_root/\n",
      "-rw-------  1 app  app  3.9K Jan 25 20:38 livyConf7692773439672439207.properties\n",
      "drwx------  2 app  app  4.0K Jan 25 20:38 rsc-tmp6226432662781460713/\n",
      "drwxr-xr-x  2 app  app  4.0K Jan  9 13:12 spark/\n",
      "drwx------  4 app  app  4.0K Jan 25 20:38 spark-0467685f-784e-4447-a714-b2e038f51229/\n",
      "drwx------ 13 app  app  4.0K Jan 25 20:38 spark7211444953516539719/\n",
      "drwxr-xr-x  2 app  app  4.0K Jan 25 20:38 spark-bde2484b-d23d-42ee-9790-4f3d2a880c46/\n",
      "drwx------  2 app  app  4.0K Jan 25 20:38 tmp_tadnnrj/\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "ls -lahF ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97109de1-6baf-47f1-a0f0-fd48aae9a6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "$ command exited with code 42"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "bash -c 'echo foo && exit 42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "336cf21a-049e-494b-8aca-bbad2dcb07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert shell_output == 'foo\\n'\n",
    "assert shell_returncode == 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33add3-c506-4a0a-b40f-02278b554530",
   "metadata": {},
   "source": [
    "## Sending local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd00633-fc4d-4d35-9625-c649c6324a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28K\n",
      "drwxrwxr-x  4 app app 4.0K Jan 25 20:38 ./\n",
      "drwxrwxr-x 16 app app 4.0K Jan 16 19:08 ../\n",
      "drwxr-xr-x  2 app app 4.0K Jan 13 02:06 .ipynb_checkpoints/\n",
      "-rw-rw-r--  1 app app  11K Jan 25 20:38 magics.ipynb\n",
      "drwxrwxr-x  3 app app 4.0K Jan  9 14:41 sample-dir/\n"
     ]
    }
   ],
   "source": [
    "%local !ls -lahF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7c16f76-a199-4c4b-bbf1-76dbfe9827a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded magics.ipynb to /tmp/magics.ipynb"
     ]
    }
   ],
   "source": [
    "%send_path_to_spark -p magics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c00b15-92bb-4256-b8c5-8972dc68867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-------  1 app  app   11K Jan 25 20:39 magics.ipynb\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "ls -lahF | grep magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75bb149a-e7d9-498b-afe0-ea34b7b255f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert 'magics.ipynb' in shell_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffb420-c3b7-4867-9d70-ee63fa6af659",
   "metadata": {},
   "source": [
    "## Sending local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "464567bf-bd72-467d-b0e3-8f754b75ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample-dir/\n",
      "sample-dir/inner\n",
      "sample-dir/inner/bar.txt\n",
      "sample-dir/foo.txt\n"
     ]
    }
   ],
   "source": [
    "%local !find sample-dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "739881b0-f362-4b16-b090-bb02403b1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded sample-dir to /tmp/sample-dir"
     ]
    }
   ],
   "source": [
    "%send_path_to_spark -p sample-dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81f65cad-1264-4cc7-befd-2054f7081af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff2d46be-e9ac-4bd0-ba23-823ae14ec601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/sample-dir\n",
      "/tmp/sample-dir/inner\n",
      "/tmp/sample-dir/inner/bar.txt\n",
      "/tmp/sample-dir/foo.txt\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "find \"$PWD/sample-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed081aa9-f3cf-4655-a94b-6d0c21e40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert 'sample-dir/' in shell_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602cd86-34ab-4a64-b785-f70616557e28",
   "metadata": {},
   "source": [
    "## Following session logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "165a6270-434f-4232-9e8d-67d053896c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "25/01/25 20:38:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/25 20:38:36 INFO RSCDriver: Connecting to: livy.docker.internal:10000\n",
      "25/01/25 20:38:36 INFO RSCDriver: Starting RPC server...\n",
      "25/01/25 20:38:36 INFO RpcServer: Connected to the port 10001\n",
      "25/01/25 20:38:36 WARN ClientConf: Your hostname, livy, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "25/01/25 20:38:36 WARN ClientConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "25/01/25 20:38:37 INFO RSCDriver: Received job request 4e569d44-8d30-4048-9b52-75e75e836832\n",
      "25/01/25 20:38:37 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "25/01/25 20:38:41 INFO SparkEntries: Starting Spark context...\n",
      "25/01/25 20:38:41 INFO SparkContext: Running Spark version 3.2.1\n",
      "25/01/25 20:38:41 INFO ResourceUtils: ==============================================================\n",
      "25/01/25 20:38:41 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/25 20:38:41 INFO ResourceUtils: ==============================================================\n",
      "25/01/25 20:38:41 INFO SparkContext: Submitted application: livy-session-0\n",
      "25/01/25 20:38:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/25 20:38:41 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "25/01/25 20:38:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/25 20:38:41 INFO SecurityManager: Changing view acls to: app\n",
      "25/01/25 20:38:41 INFO SecurityManager: Changing modify acls to: app\n",
      "25/01/25 20:38:41 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/25 20:38:41 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/25 20:38:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(app); groups with view permissions: Set(); users  with modify permissions: Set(app); groups with modify permissions: Set()\n",
      "25/01/25 20:38:41 INFO Utils: Successfully started service 'sparkDriver' on port 33029.\n",
      "25/01/25 20:38:41 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/25 20:38:41 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/25 20:38:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/25 20:38:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/25 20:38:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/25 20:38:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c96a4538-43d5-4b7e-9bb0-da3bb68ffbc6\n",
      "25/01/25 20:38:41 INFO MemoryStore: MemoryStore started with capacity 400.0 MiB\n",
      "25/01/25 20:38:41 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/25 20:38:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/01/25 20:38:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://localhost:4040\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-common-4.1.86.Final.jar at spark://livy:33029/jars/netty-common-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/kryo-shaded-4.0.2.jar at spark://livy:33029/jars/kryo-shaded-4.0.2.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/livy-rsc-0.8.0-incubating.jar at spark://livy:33029/jars/livy-rsc-0.8.0-incubating.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/objenesis-2.5.1.jar at spark://livy:33029/jars/objenesis-2.5.1.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-sctp-4.1.86.Final.jar at spark://livy:33029/jars/netty-transport-sctp-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-smtp-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-smtp-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-4.1.86.Final.jar at spark://livy:33029/jars/netty-resolver-dns-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-redis-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-redis-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-ssl-ocsp-4.1.86.Final.jar at spark://livy:33029/jars/netty-handler-ssl-ocsp-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-aarch_64.jar at spark://livy:33029/jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-aarch_64.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-all-4.1.86.Final.jar at spark://livy:33029/jars/netty-all-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-classes-epoll-4.1.86.Final.jar at spark://livy:33029/jars/netty-transport-classes-epoll-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-proxy-4.1.86.Final.jar at spark://livy:33029/jars/netty-handler-proxy-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-kqueue-4.1.86.Final-osx-aarch_64.jar at spark://livy:33029/jars/netty-transport-native-kqueue-4.1.86.Final-osx-aarch_64.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-http-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-http-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-xml-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-xml-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-rxtx-4.1.86.Final.jar at spark://livy:33029/jars/netty-transport-rxtx-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-classes-macos-4.1.86.Final.jar at spark://livy:33029/jars/netty-resolver-dns-classes-macos-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-stomp-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-stomp-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-classes-kqueue-4.1.86.Final.jar at spark://livy:33029/jars/netty-transport-classes-kqueue-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-kqueue-4.1.86.Final-osx-x86_64.jar at spark://livy:33029/jars/netty-transport-native-kqueue-4.1.86.Final-osx-x86_64.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/minlog-1.3.0.jar at spark://livy:33029/jars/minlog-1.3.0.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/livy-api-0.8.0-incubating.jar at spark://livy:33029/jars/livy-api-0.8.0-incubating.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-memcache-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-memcache-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-socks-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-socks-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-haproxy-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-haproxy-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-dns-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-dns-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-4.1.86.Final.jar at spark://livy:33029/jars/netty-resolver-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-buffer-4.1.86.Final.jar at spark://livy:33029/jars/netty-buffer-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-unix-common-4.1.86.Final.jar at spark://livy:33029/jars/netty-transport-native-unix-common-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-4.1.86.Final.jar at spark://livy:33029/jars/netty-transport-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-http2-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-http2-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-x86_64.jar at spark://livy:33029/jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-x86_64.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-epoll-4.1.86.Final-linux-aarch_64.jar at spark://livy:33029/jars/netty-transport-native-epoll-4.1.86.Final-linux-aarch_64.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-4.1.86.Final.jar at spark://livy:33029/jars/netty-handler-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar at spark://livy:33029/jars/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-udt-4.1.86.Final.jar at spark://livy:33029/jars/netty-transport-udt-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-mqtt-4.1.86.Final.jar at spark://livy:33029/jars/netty-codec-mqtt-4.1.86.Final.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/commons-codec-1.9.jar at spark://livy:33029/jars/commons-codec-1.9.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File kryo-shaded-4.0.2.jar was already registered with a different path (old path = /etc/livy/rsc-jars/kryo-shaded-4.0.2.jar, new path = /etc/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/01/25 20:38:41 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/objenesis-2.5.1.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File objenesis-2.5.1.jar was already registered with a different path (old path = /etc/livy/rsc-jars/objenesis-2.5.1.jar, new path = /etc/livy/repl_2.12-jars/objenesis-2.5.1.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/01/25 20:38:41 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/minlog-1.3.0.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File minlog-1.3.0.jar was already registered with a different path (old path = /etc/livy/rsc-jars/minlog-1.3.0.jar, new path = /etc/livy/repl_2.12-jars/minlog-1.3.0.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/livy-repl_2.12-0.8.0-incubating.jar at spark://livy:33029/jars/livy-repl_2.12-0.8.0-incubating.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/livy-core_2.12-0.8.0-incubating.jar at spark://livy:33029/jars/livy-core_2.12-0.8.0-incubating.jar with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO SparkContext: Added file file:///etc/spark/python/lib/pyspark.zip at spark://livy:33029/files/pyspark.zip with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO Utils: Copying /etc/spark/python/lib/pyspark.zip to /tmp/spark-0467685f-784e-4447-a714-b2e038f51229/userFiles-ab833d73-b62c-4005-8965-891d0435b888/pyspark.zip\n",
      "25/01/25 20:38:41 INFO SparkContext: Added file file:///etc/spark/python/lib/py4j-0.10.9.3-src.zip at spark://livy:33029/files/py4j-0.10.9.3-src.zip with timestamp 1737837521155\n",
      "25/01/25 20:38:41 INFO Utils: Copying /etc/spark/python/lib/py4j-0.10.9.3-src.zip to /tmp/spark-0467685f-784e-4447-a714-b2e038f51229/userFiles-ab833d73-b62c-4005-8965-891d0435b888/py4j-0.10.9.3-src.zip\n",
      "25/01/25 20:38:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://master.docker.internal:7077...\n",
      "25/01/25 20:38:42 INFO TransportClientFactory: Successfully created connection to master.docker.internal/172.20.0.2:7077 after 8 ms (0 ms spent in bootstraps)\n",
      "25/01/25 20:38:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250125203842-0000\n",
      "25/01/25 20:38:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43925.\n",
      "25/01/25 20:38:42 INFO NettyBlockTransferService: Server created on livy:43925\n",
      "25/01/25 20:38:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/25 20:38:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, livy, 43925, None)\n",
      "25/01/25 20:38:42 INFO BlockManagerMasterEndpoint: Registering block manager livy:43925 with 400.0 MiB RAM, BlockManagerId(driver, livy, 43925, None)\n",
      "25/01/25 20:38:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, livy, 43925, None)\n",
      "25/01/25 20:38:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, livy, 43925, None)\n",
      "25/01/25 20:38:42 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25/01/25 20:38:42 INFO SparkEntries: Spark context finished initialization in 1497ms\n",
      "25/01/25 20:38:42 INFO SparkEntries: Created Spark session.\n",
      "25/01/25 20:38:50 INFO SparkEntries: Created SQLContext.\n",
      "25/01/25 20:39:46 INFO RSCDriver: Received job request 5ba59a49-b9ec-451f-9249-d7ab45438fc5\n",
      "25/01/25 20:39:46 INFO SparkContext: Added file file:/home/app/.livy-sessions/ee633d49-88ba-439c-9e6b-3829768989d1/upload-file-02d847fd-1674-49df-9d8c-eb61ff115235.0 at spark://livy:33029/files/upload-file-02d847fd-1674-49df-9d8c-eb61ff115235.0 with timestamp 1737837586837\n",
      "25/01/25 20:39:46 INFO Utils: Copying /home/app/.livy-sessions/ee633d49-88ba-439c-9e6b-3829768989d1/upload-file-02d847fd-1674-49df-9d8c-eb61ff115235.0 to /tmp/spark-0467685f-784e-4447-a714-b2e038f51229/userFiles-ab833d73-b62c-4005-8965-891d0435b888/upload-file-02d847fd-1674-49df-9d8c-eb61ff115235.0\n",
      "25/01/25 20:39:46 WARN SparkContext: The path file:/home/app/.livy-sessions/ee633d49-88ba-439c-9e6b-3829768989d1/upload-file-02d847fd-1674-49df-9d8c-eb61ff115235.0 has been added already. Overwriting of added paths is not supported in the current version.\n",
      "25/01/25 20:39:49 INFO RSCDriver: Received job request a1742981-e144-4174-a203-47b2ec36cd17\n",
      "25/01/25 20:39:49 INFO SparkContext: Added file file:/home/app/.livy-sessions/ee633d49-88ba-439c-9e6b-3829768989d1/upload-file-80a55edf-4b9f-4e83-aa8b-15bb6cf652d5.0 at spark://livy:33029/files/upload-file-80a55edf-4b9f-4e83-aa8b-15bb6cf652d5.0 with timestamp 1737837589099\n",
      "25/01/25 20:39:49 INFO Utils: Copying /home/app/.livy-sessions/ee633d49-88ba-439c-9e6b-3829768989d1/upload-file-80a55edf-4b9f-4e83-aa8b-15bb6cf652d5.0 to /tmp/spark-0467685f-784e-4447-a714-b2e038f51229/userFiles-ab833d73-b62c-4005-8965-891d0435b888/upload-file-80a55edf-4b9f-4e83-aa8b-15bb6cf652d5.0\n",
      "25/01/25 20:39:49 WARN SparkContext: The path file:/home/app/.livy-sessions/ee633d49-88ba-439c-9e6b-3829768989d1/upload-file-80a55edf-4b9f-4e83-aa8b-15bb6cf652d5.0 has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d8e857-6c1c-47d8-ba47-da936761ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new logs"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2e594ac-505f-42f1-948b-ca47f317d381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc._gateway.jvm.java.lang.System.err.println('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d262c71-789d-40c1-9970-fedf4443a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41290b65-3939-460b-9250-3256cfb79fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "assert 'Hello World' in '\\n'.join(logs_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4707ee-019b-4142-b2a2-a20be47c38c0",
   "metadata": {},
   "source": [
    "## Making sure it kills running sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d23dcae6-06ef-4011-9bd9-da0c6273116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {}, 'name': 'test-1', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\": \"test-1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88729cd9-e670-4696-b88d-9a7cbbfc794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'name': 'test-1', 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'idle', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'ttl': None, 'driverMemory': None, 'driverCores': 0, 'executorMemory': None, 'executorCores': 0, 'conf': {}, 'archives': [], 'files': [], 'heartbeatTimeoutInSecond': 0, 'jars': [], 'numExecutors': 0, 'pyFiles': [], 'queue': None}\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "from livy_uploads.magics import get_session\n",
    "\n",
    "info = get_session().refresh()\n",
    "print(info)\n",
    "\n",
    "initial_id = info['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99e32b07-a0b9-46d1-a77e-c2c335cc5c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {}, 'name': 'test-1', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\": \"test-1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97025039-1045-41f2-bed5-c9dd404dc555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2, 'name': 'test-1', 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'idle', 'kind': 'pyspark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'ttl': None, 'driverMemory': None, 'driverCores': 0, 'executorMemory': None, 'executorCores': 0, 'conf': {}, 'archives': [], 'files': [], 'heartbeatTimeoutInSecond': 0, 'jars': [], 'numExecutors': 0, 'pyFiles': [], 'queue': None}\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "from livy_uploads.magics import get_session\n",
    "\n",
    "info = get_session().refresh()\n",
    "print(info)\n",
    "\n",
    "other_id = info['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28df5d4e-f17b-451c-87e4-9754687f4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert other_id != initial_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8546c578-ae02-4e08-a694-173c5af7e298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LivySession(url=http://livy.docker.internal:8998, id=2, name=test-1, state=idle, app_id=None, ui_url=None, driver_url=None)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "from livy_uploads.magics import get_session\n",
    "from livy_uploads.session import LivySession\n",
    "\n",
    "endpoint = get_session()\n",
    "sessions = list(LivySession.list(endpoint))\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "357bc037-1fa2-4f00-9b51-eb73b31caa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert len(sessions) == 1\n",
    "\n",
    "session = sessions[0]\n",
    "\n",
    "assert session.session_name == 'test-1'\n",
    "assert session.session_id == other_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee719b1-3bb5-4a56-b639-1c43a978d0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
