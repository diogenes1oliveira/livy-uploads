{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7539dc5-46e6-4740-890e-0b233f8a3534",
   "metadata": {},
   "source": [
    "## Load the extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddde0b3a-a5b3-4e9c-8a97-0fe590e79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext livy_uploads.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8270623-2d54-4bdb-9c8d-80e76bdecf5f",
   "metadata": {},
   "source": [
    "## Fetching remote variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46dfcbed-0a59-4533-bf3c-a1922ae663ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2610be92b141c698507049a451ded9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f9538aa36347e3873de92acf19891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HiveContext', 'StreamingContext', '__builtins__', 'cloudpickle', 'sc', 'spark', 'sqlContext']"
     ]
    }
   ],
   "source": [
    "print(list(sorted(globals())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1a1a1a-6777-4e4b-a5c0-dee6c15657cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__session__', '__spec__', '_dh', '_i', '_i1', '_i2', '_i3', '_ih', '_ii', '_iii', '_oh', 'display_dataframe', 'exit', 'get_ipython', 'ip', 'open', 'quit']\n"
     ]
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "print(list(sorted(globals())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46582d08-4d7d-45a5-9dab-a59ae4a037f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38c2af6772e4dd7afd3b854fb91f14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2025, 1, 13, 21, 41, 5, 968099, tzinfo=datetime.timezone(datetime.timedelta(0), 'UTC'))"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().astimezone()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89bdda1f-5e64-4dd4-ace7-aa7ca1349910",
   "metadata": {},
   "outputs": [],
   "source": [
    "%get_obj_from_spark -n now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc84d9e-d1c2-4cc2-8f5b-a999e53d9a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 1, 13, 21, 41, 5, 968099, tzinfo=datetime.timezone(datetime.timedelta(0), 'UTC'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492ba5f0-1df9-472e-8bd9-e06d7af2b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "delta = (datetime.now().astimezone() - now)\n",
    "assert delta.total_seconds() < 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e7b51-1a37-45ea-acc0-7be7ac355d3f",
   "metadata": {},
   "source": [
    "## Sending local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343468f8-6c26-4439-abf4-b02525785cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "foo = {2, 3, 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43925e6-ed31-4421-ad1a-d431d7d8fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%send_obj_to_spark -n foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7a1719-7cef-4e3c-be92-f0c25211054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236b10101d9841f6af33194475aa226b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo_total = sum(foo)\n",
    "\n",
    "assert foo == {2, 3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49925f03-75bc-4fdc-8040-5e6f7bf0a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%get_obj_from_spark -n foo_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab5942b-943d-4610-90f9-ed88f0bb07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "assert foo_total == 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaa787-5d90-4a70-86bd-fd0224f27718",
   "metadata": {},
   "source": [
    "## Running commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89dabe55-534a-4f6d-a6a4-641df6c322df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72K\n",
      "drwxrwxrwt  1 root root 4.0K Jan 13 21:41 ./\n",
      "drwxr-xr-x  1 root root 4.0K Jan 13 21:40 ../\n",
      "-rw-------  1 app  app   22K Jan 13 21:41 7508097095021627985\n",
      "drwxr-xr-x  2 app  app  4.0K Jan 13 21:40 blockmgr-5da1f2f5-9cf4-4133-aa6c-723220ff70c7/\n",
      "drwxr-xr-x  2 app  app  4.0K Jan 13 21:40 hsperfdata_app/\n",
      "drwxr-xr-x  2 root root 4.0K Jul 25 17:21 hsperfdata_root/\n",
      "-rw-------  1 app  app  3.8K Jan 13 21:40 livyConf9150726197463519429.properties\n",
      "drwx------  2 app  app  4.0K Jan 13 21:40 rsc-tmp320697556134094159/\n",
      "drwxr-xr-x  2 app  app  4.0K Jan  9 13:12 spark/\n",
      "drwxr-xr-x  2 app  app  4.0K Jan 13 21:40 spark-20228738-2c2e-43ed-a551-f23e6ae0d75d/\n",
      "drwx------  4 app  app  4.0K Jan 13 21:41 spark-2e6eab2e-136f-48bc-889a-163aebc379b0/\n",
      "drwx------ 13 app  app  4.0K Jan 13 21:41 spark5210915360665312650/\n",
      "drwx------  2 app  app  4.0K Jan 13 21:41 tmp5xtg6l6b/\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "ls -lahF ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97109de1-6baf-47f1-a0f0-fd48aae9a6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "$ command exited with code 42"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "bash -c 'echo foo && exit 42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "336cf21a-049e-494b-8aca-bbad2dcb07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert shell_output == 'foo\\n'\n",
    "assert shell_returncode == 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33add3-c506-4a0a-b40f-02278b554530",
   "metadata": {},
   "source": [
    "## Sending local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd00633-fc4d-4d35-9625-c649c6324a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28K\n",
      "drwxrwxr-x  4 app app 4.0K Jan 13 21:40 ./\n",
      "drwxrwxr-x 16 app app 4.0K Jan 13 02:29 ../\n",
      "drwxr-xr-x  2 app app 4.0K Jan 13 02:06 .ipynb_checkpoints/\n",
      "-rw-rw-r--  1 app app 8.1K Jan 13 21:40 magics.ipynb\n",
      "drwxrwxr-x  3 app app 4.0K Jan  9 14:41 sample-dir/\n"
     ]
    }
   ],
   "source": [
    "%local !ls -lahF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c16f76-a199-4c4b-bbf1-76dbfe9827a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded magics.ipynb to /tmp/magics.ipynb"
     ]
    }
   ],
   "source": [
    "%send_path_to_spark -p magics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62c00b15-92bb-4256-b8c5-8972dc68867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-------  1 app  app  8.1K Jan 13 21:41 magics.ipynb\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "ls -lahF | grep magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75bb149a-e7d9-498b-afe0-ea34b7b255f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert 'magics.ipynb' in shell_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffb420-c3b7-4867-9d70-ee63fa6af659",
   "metadata": {},
   "source": [
    "## Sending local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "464567bf-bd72-467d-b0e3-8f754b75ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample-dir/\n",
      "sample-dir/inner\n",
      "sample-dir/inner/bar.txt\n",
      "sample-dir/foo.txt\n"
     ]
    }
   ],
   "source": [
    "%local !find sample-dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "739881b0-f362-4b16-b090-bb02403b1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded sample-dir to /tmp/sample-dir"
     ]
    }
   ],
   "source": [
    "%send_path_to_spark -p sample-dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81f65cad-1264-4cc7-befd-2054f7081af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff2d46be-e9ac-4bd0-ba23-823ae14ec601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/sample-dir\n",
      "/tmp/sample-dir/inner\n",
      "/tmp/sample-dir/inner/bar.txt\n",
      "/tmp/sample-dir/foo.txt\n",
      "$ command exited with code 0"
     ]
    }
   ],
   "source": [
    "%%shell_command\n",
    "\n",
    "find \"$PWD/sample-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed081aa9-f3cf-4655-a94b-6d0c21e40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "assert 'sample-dir/' in shell_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602cd86-34ab-4a64-b785-f70616557e28",
   "metadata": {},
   "source": [
    "## Following session logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165a6270-434f-4232-9e8d-67d053896c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "\n",
      "stderr: \n",
      "25/01/13 21:40:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/13 21:40:53 INFO RSCDriver: Connecting to: livy.docker.internal:10000\n",
      "25/01/13 21:40:53 INFO RSCDriver: Starting RPC server...\n",
      "25/01/13 21:40:53 INFO RpcServer: Connected to the port 10001\n",
      "25/01/13 21:40:53 WARN ClientConf: Your hostname, livy, resolves to a loopback address, but we couldn't find any external IP address!\n",
      "25/01/13 21:40:53 WARN ClientConf: Set livy.rsc.rpc.server.address if you need to bind to another address.\n",
      "25/01/13 21:40:54 INFO RSCDriver: Received job request 25ea3043-c4b4-4a92-af2f-a1700ed04c14\n",
      "25/01/13 21:40:54 INFO RSCDriver: SparkContext not yet up, queueing job request.\n",
      "25/01/13 21:40:57 INFO SparkEntries: Starting Spark context...\n",
      "25/01/13 21:40:57 INFO SparkContext: Running Spark version 3.2.1\n",
      "25/01/13 21:40:57 INFO ResourceUtils: ==============================================================\n",
      "25/01/13 21:40:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/13 21:40:57 INFO ResourceUtils: ==============================================================\n",
      "25/01/13 21:40:57 INFO SparkContext: Submitted application: livy-session-0\n",
      "25/01/13 21:40:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/13 21:40:57 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "25/01/13 21:40:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/13 21:40:57 INFO SecurityManager: Changing view acls to: app\n",
      "25/01/13 21:40:57 INFO SecurityManager: Changing modify acls to: app\n",
      "25/01/13 21:40:57 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/13 21:40:57 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/13 21:40:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(app); groups with view permissions: Set(); users  with modify permissions: Set(app); groups with modify permissions: Set()\n",
      "25/01/13 21:40:58 INFO Utils: Successfully started service 'sparkDriver' on port 41553.\n",
      "25/01/13 21:40:58 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/13 21:40:58 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/13 21:40:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/13 21:40:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/13 21:40:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/13 21:40:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5da1f2f5-9cf4-4133-aa6c-723220ff70c7\n",
      "25/01/13 21:40:58 INFO MemoryStore: MemoryStore started with capacity 400.0 MiB\n",
      "25/01/13 21:40:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/13 21:40:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/01/13 21:40:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://localhost:4040\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-common-4.1.86.Final.jar at spark://livy:41553/jars/netty-common-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/kryo-shaded-4.0.2.jar at spark://livy:41553/jars/kryo-shaded-4.0.2.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/livy-rsc-0.8.0-incubating.jar at spark://livy:41553/jars/livy-rsc-0.8.0-incubating.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/objenesis-2.5.1.jar at spark://livy:41553/jars/objenesis-2.5.1.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-sctp-4.1.86.Final.jar at spark://livy:41553/jars/netty-transport-sctp-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-smtp-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-smtp-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-4.1.86.Final.jar at spark://livy:41553/jars/netty-resolver-dns-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-redis-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-redis-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-ssl-ocsp-4.1.86.Final.jar at spark://livy:41553/jars/netty-handler-ssl-ocsp-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-aarch_64.jar at spark://livy:41553/jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-aarch_64.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-all-4.1.86.Final.jar at spark://livy:41553/jars/netty-all-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-classes-epoll-4.1.86.Final.jar at spark://livy:41553/jars/netty-transport-classes-epoll-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-proxy-4.1.86.Final.jar at spark://livy:41553/jars/netty-handler-proxy-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-kqueue-4.1.86.Final-osx-aarch_64.jar at spark://livy:41553/jars/netty-transport-native-kqueue-4.1.86.Final-osx-aarch_64.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-http-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-http-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-xml-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-xml-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-rxtx-4.1.86.Final.jar at spark://livy:41553/jars/netty-transport-rxtx-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-classes-macos-4.1.86.Final.jar at spark://livy:41553/jars/netty-resolver-dns-classes-macos-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-stomp-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-stomp-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-classes-kqueue-4.1.86.Final.jar at spark://livy:41553/jars/netty-transport-classes-kqueue-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-kqueue-4.1.86.Final-osx-x86_64.jar at spark://livy:41553/jars/netty-transport-native-kqueue-4.1.86.Final-osx-x86_64.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/minlog-1.3.0.jar at spark://livy:41553/jars/minlog-1.3.0.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/livy-api-0.8.0-incubating.jar at spark://livy:41553/jars/livy-api-0.8.0-incubating.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-memcache-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-memcache-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-socks-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-socks-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-haproxy-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-haproxy-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-dns-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-dns-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-4.1.86.Final.jar at spark://livy:41553/jars/netty-resolver-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-buffer-4.1.86.Final.jar at spark://livy:41553/jars/netty-buffer-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-unix-common-4.1.86.Final.jar at spark://livy:41553/jars/netty-transport-native-unix-common-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-4.1.86.Final.jar at spark://livy:41553/jars/netty-transport-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-http2-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-http2-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-x86_64.jar at spark://livy:41553/jars/netty-resolver-dns-native-macos-4.1.86.Final-osx-x86_64.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-epoll-4.1.86.Final-linux-aarch_64.jar at spark://livy:41553/jars/netty-transport-native-epoll-4.1.86.Final-linux-aarch_64.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-handler-4.1.86.Final.jar at spark://livy:41553/jars/netty-handler-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar at spark://livy:41553/jars/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-transport-udt-4.1.86.Final.jar at spark://livy:41553/jars/netty-transport-udt-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/rsc-jars/netty-codec-mqtt-4.1.86.Final.jar at spark://livy:41553/jars/netty-codec-mqtt-4.1.86.Final.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/commons-codec-1.9.jar at spark://livy:41553/jars/commons-codec-1.9.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File kryo-shaded-4.0.2.jar was already registered with a different path (old path = /etc/livy/rsc-jars/kryo-shaded-4.0.2.jar, new path = /etc/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/01/13 21:40:58 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/objenesis-2.5.1.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File objenesis-2.5.1.jar was already registered with a different path (old path = /etc/livy/rsc-jars/objenesis-2.5.1.jar, new path = /etc/livy/repl_2.12-jars/objenesis-2.5.1.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/01/13 21:40:58 ERROR SparkContext: Failed to add file:///etc/livy/repl_2.12-jars/minlog-1.3.0.jar to Spark environment\n",
      "java.lang.IllegalArgumentException: requirement failed: File minlog-1.3.0.jar was already registered with a different path (old path = /etc/livy/rsc-jars/minlog-1.3.0.jar, new path = /etc/livy/repl_2.12-jars/minlog-1.3.0.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.rpc.netty.NettyStreamManager.addJar(NettyStreamManager.scala:81)\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1941)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)\n",
      "\tat org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:52)\n",
      "\tat org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:66)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:71)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.$anonfun$start$1(SparkInterpreter.scala:85)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:342)\n",
      "\tat org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:60)\n",
      "\tat org.apache.livy.repl.Session.$anonfun$start$1(Session.scala:128)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/livy-repl_2.12-0.8.0-incubating.jar at spark://livy:41553/jars/livy-repl_2.12-0.8.0-incubating.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added JAR file:///etc/livy/repl_2.12-jars/livy-core_2.12-0.8.0-incubating.jar at spark://livy:41553/jars/livy-core_2.12-0.8.0-incubating.jar with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO SparkContext: Added file file:///etc/spark/python/lib/pyspark.zip at spark://livy:41553/files/pyspark.zip with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO Utils: Copying /etc/spark/python/lib/pyspark.zip to /tmp/spark-2e6eab2e-136f-48bc-889a-163aebc379b0/userFiles-6c6d8afa-0456-4e13-b700-24084ace3c9c/pyspark.zip\n",
      "25/01/13 21:40:58 INFO SparkContext: Added file file:///etc/spark/python/lib/py4j-0.10.9.3-src.zip at spark://livy:41553/files/py4j-0.10.9.3-src.zip with timestamp 1736804457801\n",
      "25/01/13 21:40:58 INFO Utils: Copying /etc/spark/python/lib/py4j-0.10.9.3-src.zip to /tmp/spark-2e6eab2e-136f-48bc-889a-163aebc379b0/userFiles-6c6d8afa-0456-4e13-b700-24084ace3c9c/py4j-0.10.9.3-src.zip\n",
      "25/01/13 21:40:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://master.docker.internal:7077...\n",
      "25/01/13 21:40:58 INFO TransportClientFactory: Successfully created connection to master.docker.internal/172.22.0.2:7077 after 9 ms (0 ms spent in bootstraps)\n",
      "25/01/13 21:40:59 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250113214059-0000\n",
      "25/01/13 21:40:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37551.\n",
      "25/01/13 21:40:59 INFO NettyBlockTransferService: Server created on livy:37551\n",
      "25/01/13 21:40:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/13 21:40:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, livy, 37551, None)\n",
      "25/01/13 21:40:59 INFO BlockManagerMasterEndpoint: Registering block manager livy:37551 with 400.0 MiB RAM, BlockManagerId(driver, livy, 37551, None)\n",
      "25/01/13 21:40:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, livy, 37551, None)\n",
      "25/01/13 21:40:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, livy, 37551, None)\n",
      "25/01/13 21:40:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25/01/13 21:40:59 INFO SparkEntries: Spark context finished initialization in 1546ms\n",
      "25/01/13 21:40:59 INFO SparkEntries: Created Spark session.\n",
      "25/01/13 21:41:05 INFO SparkEntries: Created SQLContext.\n",
      "25/01/13 21:41:07 INFO RSCDriver: Received job request c83e2076-9379-44f4-bb14-68d90ef16742\n",
      "25/01/13 21:41:07 INFO SparkContext: Added file file:/home/app/.livy-sessions/23a92c95-6ef3-4d09-99c8-6a5ed7b3fdcd/upload-file-bd03d732-72e5-476d-9e38-0205f8c01872.0 at spark://livy:41553/files/upload-file-bd03d732-72e5-476d-9e38-0205f8c01872.0 with timestamp 1736804467292\n",
      "25/01/13 21:41:07 INFO Utils: Copying /home/app/.livy-sessions/23a92c95-6ef3-4d09-99c8-6a5ed7b3fdcd/upload-file-bd03d732-72e5-476d-9e38-0205f8c01872.0 to /tmp/spark-2e6eab2e-136f-48bc-889a-163aebc379b0/userFiles-6c6d8afa-0456-4e13-b700-24084ace3c9c/upload-file-bd03d732-72e5-476d-9e38-0205f8c01872.0\n",
      "25/01/13 21:41:07 WARN SparkContext: The path file:/home/app/.livy-sessions/23a92c95-6ef3-4d09-99c8-6a5ed7b3fdcd/upload-file-bd03d732-72e5-476d-9e38-0205f8c01872.0 has been added already. Overwriting of added paths is not supported in the current version.\n",
      "25/01/13 21:41:08 INFO RSCDriver: Received job request 1a759404-cb93-4d9b-8286-db2b5e82f1ad\n",
      "25/01/13 21:41:08 INFO SparkContext: Added file file:/home/app/.livy-sessions/23a92c95-6ef3-4d09-99c8-6a5ed7b3fdcd/upload-file-1f3debd8-21c6-4517-b036-a949194abbe8.0 at spark://livy:41553/files/upload-file-1f3debd8-21c6-4517-b036-a949194abbe8.0 with timestamp 1736804468294\n",
      "25/01/13 21:41:08 INFO Utils: Copying /home/app/.livy-sessions/23a92c95-6ef3-4d09-99c8-6a5ed7b3fdcd/upload-file-1f3debd8-21c6-4517-b036-a949194abbe8.0 to /tmp/spark-2e6eab2e-136f-48bc-889a-163aebc379b0/userFiles-6c6d8afa-0456-4e13-b700-24084ace3c9c/upload-file-1f3debd8-21c6-4517-b036-a949194abbe8.0\n",
      "25/01/13 21:41:08 WARN SparkContext: The path file:/home/app/.livy-sessions/23a92c95-6ef3-4d09-99c8-6a5ed7b3fdcd/upload-file-1f3debd8-21c6-4517-b036-a949194abbe8.0 has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1d8e857-6c1c-47d8-ba47-da936761ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new logs"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2e594ac-505f-42f1-948b-ca47f317d381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c80059aabfe44a3b7bf8f35b68fc4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc._gateway.jvm.java.lang.System.err.println('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d262c71-789d-40c1-9970-fedf4443a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "%logs_follow -p 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41290b65-3939-460b-9250-3256cfb79fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "\n",
    "assert 'Hello World' in '\\n'.join(logs_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88729cd9-e670-4696-b88d-9a7cbbfc794f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
